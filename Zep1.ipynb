{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook projet_final_furet_JP.ipynb to html\n",
      "[NbConvertApp] Writing 623058 bytes to projet_final_furet_JP.html\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to html projet_final_furet_JP.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jeu de donnees\n",
    "The Global Database of Events, Language, and Tone (*GDELT*), est une initiative pour construire un catalogue de comportements et de croyances sociales à travers le monde, reliant chaque personne, organisation, lieu, dénombrement, thème, source d'information, et événement à travers la planète en un seul réseau massif qui capture ce qui se passe dans le monde, le contexte, les implications ainsi que la perception des gens sur chaque jour.\n",
    "\n",
    "\n",
    "Cette base de données a eu beaucoup d'utilisations, par exemple pour mieux comprendre l'évolution et l'impact de la crise financière du 2008 (https://arxiv.org/pdf/1403.2272v1.pdf[Bayesian dynamic financial networks with time-varying predictors]) ou analyser l'évolution des relations entre des pays impliquées dans des conflits (http://www.gao.ece.ufl.edu/GXU/fun_reading/sbp_hurst.pdf[Massive Media Event Data Analysis to Assess World-Wide Political Conflict and Instability] ).\n",
    "\n",
    "GDELT est compose par trois jeux de fichiers CSV, avec un fichier compressé par tranche de 15 minutes:\n",
    "\n",
    "* les events (https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.events?tab=schema[schema], http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf[CAMEO Ontology], http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf[documentation])\n",
    "* les mentions (https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.eventmentions[schema], http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf[documentation])\n",
    "* le graph des relations => GKG, Global Knowledge Graph (https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.gkg[schema], http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf[documentation])\n",
    "\n",
    "L'ensemble des donnees _GDELT_ sont disponibles via HTTP. Un fichier CSV _masterfilelist.txt_\n",
    "(http://data.gdeltproject.org/gdeltv2/masterfilelist.txt[Master CSV data file list]) nous permmet d'avoir la liste de tous les fichiers du jeu de donnees GDELT ainsi que l'URL pour telecharger chaque fichier.\n",
    "\n",
    "\n",
    "Pour plus d'infos sur le format des fichiers vous pouvez consulter la documentation GDELT: https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook nous allons telecharger les fichiers GDELT pour la journee de 1er janvier 2021.\n",
    "On commence par definir une function fileDownloder qui telecharge un fichier a partir d’un URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "import java.net.URL\n",
    "import java.io.File\n",
    "import java.io.File\n",
    "import java.nio.file.{Files, StandardCopyOption}\n",
    "import java.net.HttpURLConnection \n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n",
    "    val url = new URL(urlOfFileToDownload)\n",
    "    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n",
    "    connection.setConnectTimeout(5000)\n",
    "    connection.setReadTimeout(5000)\n",
    "    connection.connect()\n",
    "\n",
    "    if (connection.getResponseCode >= 400)\n",
    "        println(\"error\")\n",
    "    else\n",
    "        url #> new File(fileName) !!\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut tester cette function pour telecharger en local le masterfilelist GDELT. Sur Zeppelin AWS, ce \"local\" est dans le système de fichier Hadoop/Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"/mnt/tmp/masterfilelist.txt\") // save the list file to the Spark Master\n",
    "fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\", \"/mnt/tmp/masterfilelist_trans.txt\") // save the list file to the Spark Master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commande magique ls sur ledit répertoire local"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "//ls -lrth /mnt/tmp/ |grep masterfilelist\n",
    "ls -all /mnt/tmp/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration des accès AWS et upload des listes de fichiers sur le bucket S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import com.amazonaws.services.s3.AmazonS3Client\n",
    "import com.amazonaws.auth.BasicSessionCredentials\n",
    "    \n",
    "val AWS_ID = \"ASIAQFYNH7PYXZDPLQN4\"\n",
    "val AWS_KEY = \"VH8c1djkzD7f3eobdIULgw8/Lbuf/P3KehjcsI4q\"\n",
    "val AWS_SESSION_KEY = \"FwoGZXIvYXdzEPT//////////wEaDMr/wo9zlBbaPMoILSLQAUdm8ApuIxWsqYSs9MRz7QHNu0aTcLGooQOA+MIBQjpa3bhB0cIFZbBN2fXM+FJIAn2zVM4hYIB8NIO3ycEpteGBottEylq8kT6aHfKLzyGVzI9A2+eG8yJPrcwDyk6NNL0TrIn7kdYvgxb5H5sMjkFkkhxpo1R8ilqzBirDE3jnJLaXovxiFuLm8uBOQfJZFTU+yVg5bEsu+18Dq00Ltl/u6gZVJJOxs3KIlo0O9SQPQ0nh71hPyxiVoBx0b5DwDwOrmNWx998G4BrvElGY4n0o1MaqgAYyLfE3mQev4e2KufIjESSRpOjSSxpxbYRhvtz3buGFJba2h2oGWQdCV2C3AvUk8g==\"\n",
    "\n",
    "// la classe AmazonS3Client n'est pas serializable\n",
    "// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n",
    "@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_SESSION_KEY))\n",
    "\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.session.token\",AWS_SESSION_KEY)\n",
    "\n",
    "awsClient.putObject(\"rods3\", \"masterfilelist.txt\", new File( \"/mnt/tmp/masterfilelist.txt\") )\n",
    "awsClient.putObject(\"rods3\", \"masterfilelist_trans.txt\", new File( \"/mnt/tmp/masterfilelist_trans.txt\") )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions que le fichier a bien ete upload dans le bucket S3 via un dataframe Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val filesDF = sqlContext.read.\n",
    "                    option(\"delimiter\",\" \").\n",
    "                    option(\"infer_schema\",\"true\").\n",
    "                    csv(\"s3://rods3/masterfilelist.txt\").\n",
    "                    withColumnRenamed(\"_c0\",\"size\").\n",
    "                    withColumnRenamed(\"_c1\",\"hash\").\n",
    "                    withColumnRenamed(\"_c2\",\"url\").\n",
    "                    cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val filesDF_trans = sqlContext.read.\n",
    "                    option(\"delimiter\",\" \").\n",
    "                    option(\"infer_schema\",\"true\").\n",
    "                    csv(\"s3://rods3/masterfilelist_trans.txt\").\n",
    "                    withColumnRenamed(\"_c0\",\"size\").\n",
    "                    withColumnRenamed(\"_c1\",\"hash\").\n",
    "                    withColumnRenamed(\"_c2\",\"url\").\n",
    "                    cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "filesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "filesDF_trans.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge les fichiers qui correspondent au 1er janvier 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val sampleDF = filesDF.filter(col(\"url\").contains(\"/20210101\")).cache\n",
    "val sampleDF_trans = filesDF_trans.filter(col(\"url\").contains(\"/20210101\")).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "sampleDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge uniquement les fichiers qui correspondent au 2 janvier 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val sampleDF_2 = filesDF.filter(col(\"url\").contains(\"/20210102\")).cache\n",
    "val sampleDF_trans_2 = filesDF_trans.filter(col(\"url\").contains(\"/20210102\")).cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons télécharger charger tous ces fichiers sélectionnés ET les dezipper avant upload vers le bucket S3 via un ETL Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "object AwsClient{\n",
    "    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))\n",
    "}\n",
    "\n",
    "\n",
    "sampleDF.select(\"url\").repartition(100).foreach( r=> {\n",
    "            val URL = r.getAs[String](0)\n",
    "            val fileName = r.getAs[String](0).split(\"/\").last\n",
    "            val dir = \"/mnt/tmp/\"\n",
    "            val localFileName = dir + fileName\n",
    "            fileDownloader(URL,  localFileName)\n",
    "            val localFile = new File(localFileName)\n",
    "            val localFileUnzip = new File(localFileName.split(\".zip\")(0))\n",
    "            new ZipArchive().unZip(localFileName,dir)\n",
    "            AwsClient.s3.putObject(\"rods3\", fileName.split(\".zip\")(0).split(\"/mnt/tmp/\").last, localFileUnzip )\n",
    "            localFile.delete()\n",
    "            localFileUnzip.delete()\n",
    "            \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "object AwsClient{\n",
    "    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))\n",
    "}\n",
    "\n",
    "\n",
    "sampleDF_trans.select(\"url\").repartition(100).foreach( r=> {\n",
    "            val URL = r.getAs[String](0)\n",
    "            val fileName = r.getAs[String](0).split(\"/\").last\n",
    "            val dir = \"/mnt/tmp/\"\n",
    "            val localFileName = dir + fileName\n",
    "            fileDownloader(URL,  localFileName)\n",
    "            val localFile = new File(localFileName)\n",
    "            val localFileUnzip = new File(localFileName.split(\".zip\")(0))\n",
    "            new ZipArchive().unZip(localFileName,dir)\n",
    "            AwsClient.s3.putObject(\"rods3\", fileName.split(\".zip\")(0).split(\"/mnt/tmp/\").last, localFileUnzip )\n",
    "            localFile.delete()\n",
    "            localFileUnzip.delete()\n",
    "            \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "object AwsClient{\n",
    "    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))\n",
    "}\n",
    "\n",
    "\n",
    "sampleDF_2.select(\"url\").repartition(100).foreach( r=> {\n",
    "            val URL = r.getAs[String](0)\n",
    "            val fileName = r.getAs[String](0).split(\"/\").last\n",
    "            val dir = \"/mnt/tmp/\"\n",
    "            val localFileName = dir + fileName\n",
    "            fileDownloader(URL,  localFileName)\n",
    "            val localFile = new File(localFileName)\n",
    "            val localFileUnzip = new File(localFileName.split(\".zip\")(0))\n",
    "            new ZipArchive().unZip(localFileName,dir)\n",
    "            AwsClient.s3.putObject(\"rods3\", fileName.split(\".zip\")(0).split(\"/mnt/tmp/\").last, localFileUnzip )\n",
    "            localFile.delete()\n",
    "            localFileUnzip.delete()\n",
    "            \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "object AwsClient{\n",
    "    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))\n",
    "}\n",
    "\n",
    "\n",
    "sampleDF_trans_2.select(\"url\").repartition(100).foreach( r=> {\n",
    "            val URL = r.getAs[String](0)\n",
    "            val fileName = r.getAs[String](0).split(\"/\").last\n",
    "            val dir = \"/mnt/tmp/\"\n",
    "            val localFileName = dir + fileName\n",
    "            fileDownloader(URL,  localFileName)\n",
    "            val localFile = new File(localFileName)\n",
    "            val localFileUnzip = new File(localFileName.split(\".zip\")(0))\n",
    "            new ZipArchive().unZip(localFileName,dir)\n",
    "            AwsClient.s3.putObject(\"rods3\", fileName.split(\".zip\")(0).split(\"/mnt/tmp/\").last, localFileUnzip )\n",
    "            localFile.delete()\n",
    "            localFileUnzip.delete()\n",
    "            \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commande awscli magique"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "aws s3 ls --summarize --human-readable --recursive s3://rods3/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import java.util.zip.ZipFile\n",
    "import java.io.FileInputStream\n",
    "import java.io.FileOutputStream\n",
    "import scala.collection.JavaConversions._\n",
    "import java.io.InputStream\n",
    "import java.io.OutputStream\n",
    "import java.io.File\n",
    "import java.util.zip.ZipEntry\n",
    "\n",
    "class ZipArchive {\n",
    "\n",
    "  val BUFSIZE = 4096\n",
    "  val buffer = new Array[Byte](BUFSIZE)\n",
    "\n",
    "  def unZip(source: String, targetFolder: String) = {\n",
    "    FileIsExist(source)\n",
    "    {\n",
    "    \tval zipFile = new ZipFile(source)\n",
    "\n",
    "    \tunzipAllFile(zipFile.entries.toList, getZipEntryInputStream(zipFile)_, new File(targetFolder))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def FileIsExist(path:String)(expr: => Any) = {\n",
    "    if(new File(path).exists)\n",
    "      expr\n",
    "  }\n",
    "  \n",
    "  /*---------------------------------------------------------------------------------\n",
    "     * curry method , this methond can get the inputstream of a zip entry from zipFile\n",
    "     *---------------------------------------------------------------------------------*/\n",
    "  def getZipEntryInputStream(zipFile: ZipFile)(entry: ZipEntry) = zipFile.getInputStream(entry)\n",
    "\n",
    "  def unzipAllFile(entryList: List[ZipEntry], getInputStream: (ZipEntry) => InputStream, targetFolder: File): Boolean = {\n",
    "\n",
    "    entryList match {\n",
    "      case entry :: entries =>\n",
    "\n",
    "        if (entry.isDirectory)\n",
    "          new File(targetFolder, entry.getName).mkdirs\n",
    "        else\n",
    "          saveFile(getInputStream(entry), new FileOutputStream(new File(targetFolder, entry.getName)))\n",
    "\n",
    "        unzipAllFile(entries, getInputStream, targetFolder)\n",
    "\n",
    "      case _ =>\n",
    "        true\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /*=============================================================\n",
    "   * \n",
    "   * Read InputStream and write the data to OutputStream\n",
    "   * the recursive method is writeToFile and bufferReader\n",
    "   *\n",
    "   *=============================================================*/\n",
    "  def saveFile(fis: InputStream, fos: OutputStream) = {\n",
    "\n",
    "      /*--------------------------------------------------------------\n",
    "\t   * curry a method , the method can read data from InputStream\n",
    "\t   *--------------------------------------------------------------*/\n",
    "      def bufferReader(fis: InputStream)(buffer: Array[Byte]) = (fis.read(buffer), buffer)\n",
    "\n",
    "      /*--------------------------------------------------------------\n",
    "\t   * Write the data in the buffer to outputstream\n",
    "\t   ---------------------------------------------------------------*/\n",
    "      def writeToFile(reader: (Array[Byte]) => Tuple2[Int, Array[Byte]], fos: OutputStream): Boolean = {\n",
    "        val (length, data) = reader(buffer)\n",
    "        if (length >= 0) {\n",
    "          fos.write(data, 0, length)\n",
    "          writeToFile(reader, fos)\n",
    "        } else\n",
    "          true\n",
    "      }\n",
    "\n",
    "    try {\n",
    "      writeToFile(bufferReader(fis)_, fos)\n",
    "    } finally {\n",
    "      fis.close\n",
    "      fos.close\n",
    "    }\n",
    "  }\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
