{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook projet_final_exploration.ipynb to html\n",
      "[NbConvertApp] Writing 695846 bytes to projet_final_exploration.html\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to html projet_final_exploration.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des donnees GDELT via Spark\n",
    "Dans ce notebook nous allons commencer a explorer les donnees GDELT qu'on a stockées sur S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conf Spark et AWS, classes SCALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "// %spark.dep : Commande magic de déclaration de dépendance dans une cellue et non l'interp., avant zeppelin(0.9.0)\n",
    "\n",
    "import com.mongodb.client.{MongoClient, MongoDatabase}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import com.mongodb.spark.MongoSpark\n",
    "import org.bson.Document\n",
    "import com.mongodb.spark._\n",
    "import com.mongodb.spark.config.ReadConfig\n",
    "import org.bson.codecs.configuration.CodecRegistries.{fromProviders, fromRegistries}\n",
    "import com.mongodb.spark.config._\n",
    "\n",
    "val sparkConf = new SparkConf().setAll(Map(\n",
    "      \"spark.master\" -> \"local\",\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "      \"spark.mongodb.input.uri\" -> \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest\",\n",
    "      \"spark.mongodb.output.uri\" -> \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder()\n",
    "      .appName(\"MongoSparkConnectorIntro\")\n",
    "      .config(sparkConf)\n",
    "      .config(\"spark.mongodb.input.uri\",\"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest\") // redondant mais pour mémoire conf possible ici\n",
    "      .config(\"spark.mongodb.output.uri\",\"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.COLLtest\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import com.amazonaws.services.s3.AmazonS3Client\n",
    "import com.amazonaws.auth.BasicSessionCredentials\n",
    "    \n",
    "    \n",
    "val AWS_ID = \"ASIAQFYNH7PYXZDPLQN4\"\n",
    "val AWS_KEY = \"VH8c1djkzD7f3eobdIULgw8/Lbuf/P3KehjcsI4q\"\n",
    "val AWS_SESSION_KEY = \"FwoGZXIvYXdzEPT//////////wEaDMr/wo9zlBbaPMoILSLQAUdm8ApuIxWsqYSs9MRz7QHNu0aTcLGooQOA+MIBQjpa3bhB0cIFZbBN2fXM+FJIAn2zVM4hYIB8NIO3ycEpteGBottEylq8kT6aHfKLzyGVzI9A2+eG8yJPrcwDyk6NNL0TrIn7kdYvgxb5H5sMjkFkkhxpo1R8ilqzBirDE3jnJLaXovxiFuLm8uBOQfJZFTU+yVg5bEsu+18Dq00Ltl/u6gZVJJOxs3KIlo0O9SQPQ0nh71hPyxiVoBx0b5DwDwOrmNWx998G4BrvElGY4n0o1MaqgAYyLfE3mQev4e2KufIjESSRpOjSSxpxbYRhvtz3buGFJba2h2oGWQdCV2C3AvUk8g==\"\n",
    "\n",
    "// la classe AmazonS3Client n'est pas serializable\n",
    "// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n",
    "@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_SESSION_KEY))\n",
    "\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.session.token\",AWS_SESSION_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise des case class et un schéma/format de spark mongo pour la gestion des schémas, ainsi que (plus bas) des encoders.\n",
    "A noter que ce code a été délicat à mettre en place, mais est a priori le plus concis. A noter les erreurs de types de datas sur la BigTable de GDELT !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "case class Mention(GLOBALEVENTID: BigInt,\n",
    "                    EventTimeDate: BigInt,\n",
    "                    MentionTimeDate: BigInt,\n",
    "                    MentionType: Int,\n",
    "                    MentionSourceName: String,\n",
    "                    MentionIdentifier: String, \n",
    "                    SentenceID: Int, \n",
    "                    Actor1CharOffset: Double, \n",
    "                    Actor2CharOffset: Double,\n",
    "                    ActionCharOffset: Double,\n",
    "                    InRawText: Int,\n",
    "                    Confidence: Int,\n",
    "                    MentionDocLen: Int,\n",
    "                    MentionDocTone: Double,\n",
    "                    MentionDocTranslationInfo: String,\n",
    "                    Extras: String\n",
    "                    )\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "case class Event(GLOBALEVENTID: Int,\n",
    "                   SQLDATE: Int,\n",
    "                   MonthYear: Int,\n",
    "                   Year: Int,\n",
    "                   FractionDate: Double,\n",
    "                   Actor1Code: String,\n",
    "                   Actor1Name: String,\n",
    "                   Actor1CountryCode: String,\n",
    "                   Actor1KnownGroupCode: String,\n",
    "                   Actor1EthnicCode: String,\n",
    "                   Actor1Religion1Code: String,\n",
    "                   Actor1Religion2Code: String,\n",
    "                   Actor1Type1Code: String,\n",
    "                   Actor1Type2Code: String,\n",
    "                   Actor1Type3Code: String,\n",
    "                   Actor2Code: String,\n",
    "                   Actor2Name: String,\n",
    "                   Actor2CountryCode: String,\n",
    "                   Actor2KnownGroupCode: String,\n",
    "                   Actor2EthnicCode: String,\n",
    "                   Actor2Religion1Code: String,\n",
    "                   Actor2Religion2Code: String,\n",
    "                   Actor2Type1Code: String,\n",
    "                   Actor2Type2Code: String,\n",
    "                   Actor2Type3Code: String,\n",
    "                   IsRootEvent: Int,\n",
    "                   EventCode: String,\n",
    "                   EventBaseCode: String,\n",
    "                   EventRootCode: String,\n",
    "                   QuadClass: Int,\n",
    "                   GoldsteinScale: Double,\n",
    "                   NumMentions: Int,\n",
    "                   NumSources: Int,\n",
    "                   NumArticles: Int,\n",
    "                   AvgTone: Double,\n",
    "                   Actor1Geo_Type: Int,\n",
    "                   Actor1Geo_FullName: String,\n",
    "                   Actor1Geo_CountryCode: String,\n",
    "                   Actor1Geo_ADM1Code: String,\n",
    "                   Actor1Geo_ADM2Code: String,\n",
    "                   Actor1Geo_Lat: Double,\n",
    "                   Actor1Geo_Long: Double,\n",
    "                   Actor1Geo_FeatureID: String,\n",
    "                   Actor2Geo_Type: Int,\n",
    "                   Actor2Geo_FullName: String,\n",
    "                   Actor2Geo_CountryCode: String,\n",
    "                   Actor2Geo_ADM1Code: String,\n",
    "                   Actor2Geo_ADM2Code: String,\n",
    "                   Actor2Geo_Lat: Double,\n",
    "                   Actor2Geo_Long: Double,\n",
    "                   Actor2Geo_FeatureID: String,\n",
    "                   ActionGeo_Type: Int,\n",
    "                   ActionGeo_FullName: String,\n",
    "                   ActionGeo_CountryCode: String,\n",
    "                   ActionGeo_ADM1Code: String,\n",
    "                   ActionGeo_ADM2Code: String,\n",
    "                   ActionGeo_Lat: Double,\n",
    "                   ActionGeo_Long: Double,\n",
    "                   ActionGeo_FeatureID: String,\n",
    "                   DATEADDED: BigInt,\n",
    "                   SOURCEURL: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "case class GkG(\n",
    "    GKGRECORDID: String,\n",
    "    DATE: BigInt,\n",
    "    SourceCollectionIdentifier: Int,\n",
    "    SourceCommonName: String,\n",
    "    DocumentIdentifier: String,\n",
    "    Counts: String,\n",
    "    V2Counts: String,\n",
    "    Themes: String,\n",
    "    V2Themes: String,\n",
    "    Locations: String,\n",
    "    V2Locations: String,\n",
    "    Persons: String,\n",
    "    V2Persons: String,\n",
    "    Organizations: String,\n",
    "    V2Organizations: String,\n",
    "    V2Tone: String,\n",
    "    Dates: String,\n",
    "    GCAM: String,\n",
    "    SharingImage: String,\n",
    "    RelatedImages: String,\n",
    "    SocialImageEmbeds: String,\n",
    "    SocialVideoEmbeds: String,\n",
    "    Quotations: String,\n",
    "    AllNames: String,\n",
    "    Amounts: String,\n",
    "    TranslationInfo: String,\n",
    "    Extras: String,\n",
    "    Year: Int,\n",
    "    Month: Int,\n",
    "    Day: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Encoders\n",
    "\n",
    "val encoderSchemaEvent = Encoders.product[Event].schema \n",
    "encoderSchemaEvent.printTreeString()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val encoderSchemaMention = Encoders.product[Mention].schema \n",
    "encoderSchemaMention.printTreeString()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val encoderSchemaGkG = Encoders.product[GkG].schema \n",
    "encoderSchemaGkG.printTreeString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "//val encoderSchemaEventTrans = Encoders.product[EventTrans].schema \n",
    "//encoderSchemaEventTrans.printTreeString()\n",
    "\n",
    "//val encoderSchemaMentionTrans = Encoders.product[MentionTrans].schema \n",
    "//encoderSchemaMentionTrans.printTreeString()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentions et events traduits inférés avec un objet Structype (variante de la solution case class ci-dessus, juste pour mémoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val schemaEventTrans =  StructType(\n",
    "    Seq(\n",
    "    StructField(name = \"EventId\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"Day\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"MonthYear\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"Year\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"FractionDate\", dataType = DoubleType, nullable = false),\n",
    "\n",
    "      StructField(name = \"Actor1Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Name\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1CountryCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1KnownGroupCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1EthnicCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Religion1Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Religion2Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Type1Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Type2Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Type3Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Name\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2CountryCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2KnownGroupCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2EthnicCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Religion1Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Religion2Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Type1Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Type2Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Type3Code\", dataType = StringType, nullable = true),\n",
    "      \n",
    "      StructField(name = \"IsRootEvent\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"EventCode\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"EventBaseCode\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"EventRootCode\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"QuadClass\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"GoldsteinScale\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"NumMentions\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"NumSources\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"NumArticles\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"AvgTone\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"Actor1Geo_Type\", dataType = DoubleType, nullable = false),\n",
    "      \n",
    "      StructField(name = \"Actor1Geo_FullName\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Geo_CountryCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Geo_ADM1Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor1Geo_ADM2Code\", dataType = StringType, nullable = true),\n",
    "      \n",
    "      \n",
    "      \n",
    "      StructField(name = \"Actor1Geo_Lat\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"Actor1Geo_Long\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"Actor1Geo_FeatureID\", dataType =  StringType, nullable = true),\n",
    "     \n",
    "      StructField(name = \"Actor2Geo_Type\", dataType = IntegerType, nullable = false),\n",
    "      StructField(name = \"Actor2Geo_FullName\", dataType = StringType, nullable = true),\n",
    "      \n",
    "      \n",
    "      \n",
    "      StructField(name = \"Actor2Geo_CountryCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Geo_ADM1Code\", dataType = StringType, nullable = true),\n",
    "      \n",
    "      StructField(name = \"Actor2Geo_ADM2Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Actor2Geo_Lat\", dataType = DoubleType, nullable = false),\n",
    "      \n",
    "      \n",
    "      \n",
    "      StructField(name = \"Actor2Geo_Long\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"Actor2Geo_FeatureID\", dataType = StringType, nullable = true),\n",
    "      \n",
    "      StructField(name = \"ActionGeo_Type\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"ActionGeo_FullName\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"ActionGeo_CountryCode\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"ActionGeo_ADM1Code\", dataType = StringType, nullable = true),\n",
    "      \n",
    "      StructField(name = \"ActionGeo_ADM2Code\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"ActionGeo_Lat\", dataType = DoubleType, nullable = false),\n",
    "      \n",
    "      \n",
    "      StructField(name = \"ActionGeo_Long\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"ActionGeo_FeatureID\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"DATEADDED\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"SOURCEURL\", dataType = StringType, nullable = true)\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val schemaMentionTrans =  StructType(\n",
    "    Seq(\n",
    "      StructField(name = \"GlobalEventID\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"EventTimeDate\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"MentionTimeDate\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"MentionType\", dataType = IntegerType, nullable = false),\n",
    "      StructField(name = \"MentionSourceName\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"MentionIdentifier\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"SentenceID\", dataType = IntegerType, nullable = false),\n",
    "      StructField(name = \"Actor1CharOffset\", dataType = IntegerType, nullable = false),\n",
    "      StructField(name = \"Actor2CharOffset\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"ActionCharOffset\", dataType = IntegerType, nullable = false),\n",
    "      StructField(name = \"InRawText\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"Confidence\", dataType = IntegerType, nullable = false),\n",
    "      StructField(name = \"MentionDocLen\", dataType = IntegerType, nullable = false),\n",
    "      StructField(name = \"MentionDocTone\", dataType = DoubleType, nullable = false),\n",
    "      StructField(name = \"MentionDocTranslationInfo\", dataType = StringType, nullable = true),\n",
    "      StructField(name = \"Extras\", dataType = StringType, nullable = true)\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des buckets S3 au format .CSV et non .zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df_events = spark.read.options(Map(\"delimiter\"->\"\\t\")).schema(encoderSchemaEvent).csv(\"s3://rods3/[0-9]*.export.CSV\")\n",
    "df_events.show(2, truncate=200, vertical=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df_mentions = spark.read.options(Map(\"delimiter\"->\"\\t\")).schema(encoderSchemaMention).csv(\"s3://rods3/[0-9]*.mentions.CSV\")\n",
    "df_mentions.show(5, truncate=200, vertical=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df_gkg = spark.read.options(Map(\"delimiter\"->\"\\t\")).schema(encoderSchemaGkG).csv(\"s3://rods3/[0-9]*.gkg.csv\")\n",
    "df_gkg.show(5, truncate=200, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allègement des dataframes en fonction des features identifiées comme utiles dans le use case (requêtes demandées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df_events_light = df_events.select(\"GLOBALEVENTID\",\"SQLDATE\",\"MonthYear\",\"Year\",\"NumMentions\",\"SOURCEURL\",\"ActionGeo_CountryCode\")\n",
    "df_events_light.show(2, truncate=400, vertical=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df_mentions_light = df_mentions.select(\"GLOBALEVENTID\",\"MentionDocTranslationInfo\")\n",
    "df_mentions_light.show(2, truncate=400, vertical=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df_gkg_light = df_gkg.select(\"GKGRECORDID\",\"DATE\",\"SourceCommonName\",\"Themes\",\"Locations\",\"Persons\",\"V2Tone\")\n",
    "df_gkg_light.show(2, truncate=400, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfert vers le cluster MongoDB Atlas \n",
    "Ecriture de 3 collections dans notre BD : Events, mentions et gkg, dans leur version light faite ci-dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "//MongoSpark.save(df_events_light.write.option(\"spark.mongodb.output.uri\", \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.events?retryWrites=true&w=majority\").mode(\"overwrite\"))\n",
    "//MongoSpark.save(df_mentions_light.write.option(\"spark.mongodb.output.uri\", \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.mentions?retryWrites=true&w=majority\").mode(\"overwrite\"))\n",
    "//MongoSpark.save(df_gkg_light.write.option(\"spark.mongodb.output.uri\", \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.gkg?retryWrites=true&w=majority\").mode(\"overwrite\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture / requête de nos collections MongoDB allégées. \n",
    "2 jours de data ont été chargées : 495MB pour quota 500MB sur la version Atlas AWS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée une Mongo ReadConfig qui pointe en lectures la collection souhaitée (.../DBtest/collection_souhaitée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val readConfigEvents = ReadConfig(Map(\"uri\" ->  \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.events\", \"readPreference.name\" -> \"secondaryPreferred\"))\n",
    "val readConfigMentions = ReadConfig(Map(\"uri\" ->  \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.mentions\", \"readPreference.name\" -> \"secondaryPreferred\"))\n",
    "val readConfigGkG = ReadConfig(Map(\"uri\" ->  \"mongodb+srv://admin:admin@cluster0.ldu6v.mongodb.net/DBtest.gkg\", \"readPreference.name\" -> \"secondaryPreferred\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lit les collections du cluster Mongo Atlas avec la méthode load du connecteur MongoSpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val dfE = MongoSpark.load(spark, readConfigEvents).cache\n",
    "dfE.show(2, truncate=400, vertical=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val dfM = MongoSpark.load(spark, readConfigMentions).cache\n",
    "dfM.show(2, truncate=400, vertical=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val dfG = MongoSpark.load(spark, readConfigGkG).cache\n",
    "dfM.show(2, truncate=400, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics Spark Dataframes (pour mémoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfE.show(2, truncate = 62, vertical = true)\n",
    "dfE.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "println(s\"Nombre de lignes : ${dfE.count}\") \n",
    "println(s\"Nombre de colonnes : ${dfE.columns.length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfE.select(\"GLOBALEVENTID\",\"SOURCEURL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfE.filter(dfE(\"SOURCEURL\").like(\"%covid%\")).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfE.filter(dfE(\"SOURCEURL\").like(\"%COVID%\")).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfE.filter(dfE(\"SOURCEURL\").like(\"%covid%\"))\n",
    "    .groupBy(\"ActionGeo_CountryCode\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .withColumnRenamed(\"count\",\"nb_covid_papers\")\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_events.filter(df_events(\"SOURCEURL\").like(\"%covid%\"))\n",
    "    .groupBy(\"ActionGeo_CountryCode\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .withColumnRenamed(\"count\",\"nb_covid_papers\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réponses aux questions projet Furets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponse à la question 1 \n",
    "\"Afficher le nombre d’articles/évènements qui parlent de COVID qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article)\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val question1 = dfE.join(dfM, dfE(\"GLOBALEVENTID\")===dfM(\"GLOBALEVENTID\"))\n",
    "        .filter(dfE(\"SOURCEURL\").like(\"%covid%\"))\n",
    "        .groupBy(\"SQLDATE\",\"ActionGeo_CountryCode\", \"MentionDocTranslationInfo\")\n",
    "        .count()\n",
    "        .sort(desc(\"SQLDATE\"),desc(\"count\"))\n",
    "        .withColumnRenamed(\"count\",\"nb_covid_papers\")\n",
    "        .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci dessous la même requête mais avec TRI ASCENDANT sur la date pour vérifier que la requête ci-dessus affiche bien les deux journées!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val question1suite = dfE.join(dfM, dfE(\"GLOBALEVENTID\")===dfM(\"GLOBALEVENTID\"))\n",
    "        .filter(dfE(\"SOURCEURL\").like(\"%covid%\"))\n",
    "        .groupBy(\"SQLDATE\",\"ActionGeo_CountryCode\", \"MentionDocTranslationInfo\")\n",
    "        .count()\n",
    "        .sort(asc(\"SQLDATE\"),desc(\"count\"))\n",
    "        .withColumnRenamed(\"count\",\"nb_covid_papers\")\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variante avec SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "//CI DESSOUS NOK java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\n",
    "\n",
    "//dfE.createOrReplaceTempView(\"lesevents\")\n",
    "//dfM.createOrReplaceTempView(\"lesmentions\")\n",
    "\n",
    "//val centenarians = spark.sql(\"SELECT lesevents.SQLDATE, lesevents.ActionGeo_CountryCode, MentionDocTranslationInfo, count(*) as nb_refs_covid FROM lesevents, lesmentions WHERE (lesevents.SOURCEURL like '%COVID%' or lesevents.SOURCEURL like '%COVID%' ) and lesevents.GLOBALEVENTID=lesmentions.GLOBALEVENTID group by lesevents.SQLDATE, lesevents.ActionGeo_CountryCode, MentionDocTranslationInfo ORDER by nb_refs_covid\")\n",
    "//centenarians.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question2\n",
    "pour un pays donné en paramètre, affichez les évènements qui y ont eu place triées par le nombre de mentions (tri décroissant); permettez une agrégation par jour/mois/année"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val question2 = dfE\n",
    "        .select(\"NumMentions\",\"SQLDATE\",\"ActionGeo_CountryCode\")\n",
    "        .filter(dfE(\"ActionGeo_CountryCode\").like(\"%FR%\"))\n",
    "        .groupBy(\"SQLDATE\",\"ActionGeo_CountryCode\")\n",
    "        .agg(sum(\"NumMentions\"))\n",
    "        .withColumnRenamed(\"sum(NumMentions)\",\"NbM\")\n",
    "        .sort(desc(\"NbM\"))\n",
    "\n",
    "question2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution MongoDB pipelines\n",
    "\n",
    "When using filters with DataFrames or Spark SQL, the underlying Mongo Connector code constructs an aggregation pipeline to filter the data in MongoDB before sending it to Spark.\n",
    "https://docs.mongodb.com/spark-connector/current/scala/datasets-and-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfE.filter(dfE(\"SOURCEURL\").like(\"%covid%\")).select(dfE(\"GLOBALEVENTID\"),dfE(\"SOURCEURL\")).show(5, truncate=400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème 1 : SparkSQL donne le message suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "//CI DESSOUS NOK java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\n",
    "\n",
    "dfE.createOrReplaceTempView(\"joinQ1\")\n",
    "val centenarians = spark.sql(\"SELECT GLOBALEVENTID, SOURCEURL FROM lesevents WHERE (SOURCEURL like '%COVID%' or SOURCEURL like '%COVID%' )\"\n",
    "\n",
    ")\n",
    "centenarians.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the dataset, filtering data using MongoDB's aggregation framework may perform more efficiently than the direct use of RDD filters and dataset filters.\n",
    "https://docs.mongodb.com/spark-connector/current/scala/aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation\n",
    "\n",
    "Pass an aggregation pipeline to a MongoRDD instance to filter data and perform aggregations in MongoDB before passing documents to Spark.\n",
    "Aggregation pipelines handle null results whereas the filter methods do not. If the filter does not match any documents, the operation throws the following exception:\n",
    "ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8) java.lang.NullPointerException\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de pipeline (sans regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val rdd = MongoSpark.load(sc, readConfigEvents)  // rdd est un MongoRDD, classe pour pipelines\n",
    "val mongoRddEvents = rdd.withPipeline(Seq(Document.parse(\"{ $match: { GoldsteinScale  : { $gt : 3 } } }\")))\n",
    "println(mongoRddEvents.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous est correct : pipeline avec regex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val rdd2 = MongoSpark.load(sc, readConfigEvents)  // rdd est un MongoRDD, classe pour pipelines\n",
    "val mongoRddEvents2 = rdd.withPipeline(Seq(Document.parse(\"{ $regexMatch: { input: '$SOURCEURL', regex: /covid/i } }\")))\n",
    "println(aggregatedRdd2.count)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais malheureusement dans la version de base de Atlas les regex ne sont pas autorisés !\n",
    "Command failed with error 8000 (AtlasError): '$regexMatch is not allowed in this atlas tier' on server cluster0-shard-00-00.ldu6v.mongodb.net:27017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "pour une source de donnés passée en paramètre (gkg.SourceCommonName) affichez les thèmes, personnes, lieux dont les articles de cette sources parlent ainsi que le le nombre d’articles et le ton moyen des articles (pour chaque thème/personne/lieu); permettez une agrégation par jour/mois/année."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val gkg_light2 = df_gkg.select(\"GKGRECORDID\",\"DATE\",\"SourceCommonName\",\"Themes\",\"V2Themes\",\"Locations\",\"V2Persons\",\"Persons\",\"V2Tone\")\n",
    "gkg_light2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfG.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val gkg_light = dfG.select(\"GKGRECORDID\",\"DATE\",\"SourceCommonName\",\"Themes\",\"Locations\",\"Persons\",\"V2Tone\")\n",
    "\n",
    "gkg_light.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val gkg_2 = gkg_light.withColumn(\"avg_tone\",substring_index($\"V2Tone\",\",\",1))\n",
    "                         .filter(!($\"SourceCommonName\" === \"\"))\n",
    "                         .withColumn(\"date\",substring($\"DATE\",0,8).cast(\"Int\"))\n",
    "                         .withColumn(\"persons_arr\",split(gkg_light(\"Persons\"),\";\"))\n",
    "                         .withColumn(\"locations_arr\",split(gkg_light(\"Locations\"),\";\"))\n",
    "                         .withColumn(\"themes_arr\",split(gkg_light(\"themes\"),\";\"))\n",
    "gkg_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val gkg2_persons = gkg_2.select($\"date\",$\"GKGRECORDID\",$\"SourceCommonName\",$\"avg_tone\",explode_outer($\"persons_arr\").as(\"persons_2\"))\n",
    "val gkg2_locations = gkg_2.select($\"date\",$\"GKGRECORDID\",$\"SourceCommonName\",$\"avg_tone\",explode_outer($\"locations_arr\").as(\"locations_2\"))\n",
    "val gkg2_themes = gkg_2.select($\"date\",$\"GKGRECORDID\",$\"SourceCommonName\",$\"avg_tone\",explode_outer($\"themes_arr\").as(\"themes_2\"))\n",
    "\n",
    "\n",
    "val requete3_persons = gkg2_persons\n",
    "    .groupBy(\"date\",\"SourceCommonName\",\"persons_2\")\n",
    "    .agg(count($\"GKGRECORDID\").cast(\"int\").as(\"nombre_articles\"),avg($\"avg_tone\").as(\"ton_moyen\"))\n",
    "val requete3_locations = gkg2_locations\n",
    "        .groupBy(\"date\",\"SourceCommonName\",\"locations_2\")\n",
    "        .agg(count($\"GKGRECORDID\").cast(\"int\").as(\"nombre_articles\"),avg($\"avg_tone\").as(\"ton_moyen\"))\n",
    "val requete3_themes = gkg2_themes\n",
    "    .groupBy(\"date\",\"SourceCommonName\",\"themes_2\")\n",
    "    .agg(count($\"GKGRECORDID\").cast(\"int\").as(\"nombre_articles\"),avg($\"avg_tone\").as(\"ton_moyen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "requete3_persons.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "requete3_locations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "requete3_themes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
