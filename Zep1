

Jeu de donnees

The Global Database of Events, Language, and Tone (GDELT), est une initiative pour construire un catalogue de comportements et de croyances sociales à travers le monde, reliant chaque personne, organisation, lieu, dénombrement, thème, source d'information, et événement à travers la planète en un seul réseau massif qui capture ce qui se passe dans le monde, le contexte, les implications ainsi que la perception des gens sur chaque jour.

Cette base de données a eu beaucoup d'utilisations, par exemple pour mieux comprendre l'évolution et l'impact de la crise financière du 2008 (https://arxiv.org/pdf/1403.2272v1.pdf[Bayesian dynamic financial networks with time-varying predictors]) ou analyser l'évolution des relations entre des pays impliquées dans des conflits (http://www.gao.ece.ufl.edu/GXU/fun_reading/sbp_hurst.pdf[Massive Media Event Data Analysis to Assess World-Wide Political Conflict and Instability] ).

GDELT est compose par trois jeux de fichiers CSV, avec un fichier compressé par tranche de 15 minutes:

    les events (https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.events?tab=schema[schema], http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf[CAMEO Ontology], http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf[documentation])
    les mentions (https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.eventmentions[schema], http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf[documentation])
    le graph des relations => GKG, Global Knowledge Graph (https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.gkg[schema], http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf[documentation])

L'ensemble des donnees GDELT sont disponibles via HTTP. Un fichier CSV masterfilelist.txt (http://data.gdeltproject.org/gdeltv2/masterfilelist.txt[Master CSV data file list]) nous permmet d'avoir la liste de tous les fichiers du jeu de donnees GDELT ainsi que l'URL pour telecharger chaque fichier.

Pour plus d'infos sur le format des fichiers vous pouvez consulter la documentation GDELT: https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/

Dans ce notebook nous allons telecharger les fichiers GDELT pour la journee de 1er janvier 2021. On commence par definir une function fileDownloder qui telecharge un fichier a partir d’un URL.

import sys.process._
import java.net.URL
import java.io.File
import java.io.File
import java.nio.file.{Files, StandardCopyOption}
import java.net.HttpURLConnection 
import org.apache.spark.sql.functions._


def fileDownloader(urlOfFileToDownload: String, fileName: String) = {
    val url = new URL(urlOfFileToDownload)
    val connection = url.openConnection().asInstanceOf[HttpURLConnection]
    connection.setConnectTimeout(5000)
    connection.setReadTimeout(5000)
    connection.connect()

    if (connection.getResponseCode >= 400)
        println("error")
    else
        url #> new File(fileName) !!
}

On peut tester cette function pour telecharger en local le masterfilelist GDELT. Sur Zeppelin AWS, ce "local" est dans le système de fichier Hadoop/Spark

fileDownloader("http://data.gdeltproject.org/gdeltv2/masterfilelist.txt", "/mnt/tmp/masterfilelist.txt") // save the list file to the Spark Master
fileDownloader("http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt", "/mnt/tmp/masterfilelist_trans.txt") // save the list file to the Spark Master

Commande magique ls sur ledit répertoire local
%sh //ls -lrth /mnt/tmp/ |grep masterfilelist ls -all /mnt/tmp/

Configuration des accès AWS et upload des listes de fichiers sur le bucket S3

import com.amazonaws.services.s3.AmazonS3Client
import com.amazonaws.auth.BasicSessionCredentials
    
val AWS_ID = "ASIAQFYNH7PYXZDPLQN4"
val AWS_KEY = "VH8c1djkzD7f3eobdIULgw8/Lbuf/P3KehjcsI4q"
val AWS_SESSION_KEY = "FwoGZXIvYXdzEPT//////////wEaDMr/wo9zlBbaPMoILSLQAUdm8ApuIxWsqYSs9MRz7QHNu0aTcLGooQOA+MIBQjpa3bhB0cIFZbBN2fXM+FJIAn2zVM4hYIB8NIO3ycEpteGBottEylq8kT6aHfKLzyGVzI9A2+eG8yJPrcwDyk6NNL0TrIn7kdYvgxb5H5sMjkFkkhxpo1R8ilqzBirDE3jnJLaXovxiFuLm8uBOQfJZFTU+yVg5bEsu+18Dq00Ltl/u6gZVJJOxs3KIlo0O9SQPQ0nh71hPyxiVoBx0b5DwDwOrmNWx998G4BrvElGY4n0o1MaqgAYyLfE3mQev4e2KufIjESSRpOjSSxpxbYRhvtz3buGFJba2h2oGWQdCV2C3AvUk8g=="

// la classe AmazonS3Client n'est pas serializable
// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs
@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_SESSION_KEY))

sc.hadoopConfiguration.set("fs.s3a.access.key", AWS_ID) // mettre votre ID du fichier credentials.csv
sc.hadoopConfiguration.set("fs.s3a.secret.key", AWS_KEY) // mettre votre secret du fichier credentials.csv
sc.hadoopConfiguration.set("fs.s3a.session.token",AWS_SESSION_KEY)

awsClient.putObject("rods3", "masterfilelist.txt", new File( "/mnt/tmp/masterfilelist.txt") )
awsClient.putObject("rods3", "masterfilelist_trans.txt", new File( "/mnt/tmp/masterfilelist_trans.txt") )

Vérifions que le fichier a bien ete upload dans le bucket S3 via un dataframe Spark

import org.apache.spark.sql.SQLContext

val sqlContext = new SQLContext(sc)
val filesDF = sqlContext.read.
                    option("delimiter"," ").
                    option("infer_schema","true").
                    csv("s3://rods3/masterfilelist.txt").
                    withColumnRenamed("_c0","size").
                    withColumnRenamed("_c1","hash").
                    withColumnRenamed("_c2","url").
                    cache

import org.apache.spark.sql.SQLContext

val sqlContext = new SQLContext(sc)
val filesDF_trans = sqlContext.read.
                    option("delimiter"," ").
                    option("infer_schema","true").
                    csv("s3://rods3/masterfilelist_trans.txt").
                    withColumnRenamed("_c0","size").
                    withColumnRenamed("_c1","hash").
                    withColumnRenamed("_c2","url").
                    cache

filesDF.show(false)

filesDF_trans.show(false)

On charge les fichiers qui correspondent au 1er janvier 2021

val sampleDF = filesDF.filter(col("url").contains("/20210101")).cache
val sampleDF_trans = filesDF_trans.filter(col("url").contains("/20210101")).cache

sampleDF.show(false)

On charge uniquement les fichiers qui correspondent au 2 janvier 2021

val sampleDF_2 = filesDF.filter(col("url").contains("/20210102")).cache
val sampleDF_trans_2 = filesDF_trans.filter(col("url").contains("/20210102")).cache

Nous allons télécharger charger tous ces fichiers sélectionnés ET les dezipper avant upload vers le bucket S3 via un ETL Spark:

object AwsClient{
    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))
}


sampleDF.select("url").repartition(100).foreach( r=> {
            val URL = r.getAs[String](0)
            val fileName = r.getAs[String](0).split("/").last
            val dir = "/mnt/tmp/"
            val localFileName = dir + fileName
            fileDownloader(URL,  localFileName)
            val localFile = new File(localFileName)
            val localFileUnzip = new File(localFileName.split(".zip")(0))
            new ZipArchive().unZip(localFileName,dir)
            AwsClient.s3.putObject("rods3", fileName.split(".zip")(0).split("/mnt/tmp/").last, localFileUnzip )
            localFile.delete()
            localFileUnzip.delete()
            
})

object AwsClient{
    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))
}


sampleDF_trans.select("url").repartition(100).foreach( r=> {
            val URL = r.getAs[String](0)
            val fileName = r.getAs[String](0).split("/").last
            val dir = "/mnt/tmp/"
            val localFileName = dir + fileName
            fileDownloader(URL,  localFileName)
            val localFile = new File(localFileName)
            val localFileUnzip = new File(localFileName.split(".zip")(0))
            new ZipArchive().unZip(localFileName,dir)
            AwsClient.s3.putObject("rods3", fileName.split(".zip")(0).split("/mnt/tmp/").last, localFileUnzip )
            localFile.delete()
            localFileUnzip.delete()
            
})

object AwsClient{
    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))
}


sampleDF_2.select("url").repartition(100).foreach( r=> {
            val URL = r.getAs[String](0)
            val fileName = r.getAs[String](0).split("/").last
            val dir = "/mnt/tmp/"
            val localFileName = dir + fileName
            fileDownloader(URL,  localFileName)
            val localFile = new File(localFileName)
            val localFileUnzip = new File(localFileName.split(".zip")(0))
            new ZipArchive().unZip(localFileName,dir)
            AwsClient.s3.putObject("rods3", fileName.split(".zip")(0).split("/mnt/tmp/").last, localFileUnzip )
            localFile.delete()
            localFileUnzip.delete()
            
})

object AwsClient{
    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_SESSION_KEY))
}


sampleDF_trans_2.select("url").repartition(100).foreach( r=> {
            val URL = r.getAs[String](0)
            val fileName = r.getAs[String](0).split("/").last
            val dir = "/mnt/tmp/"
            val localFileName = dir + fileName
            fileDownloader(URL,  localFileName)
            val localFile = new File(localFileName)
            val localFileUnzip = new File(localFileName.split(".zip")(0))
            new ZipArchive().unZip(localFileName,dir)
            AwsClient.s3.putObject("rods3", fileName.split(".zip")(0).split("/mnt/tmp/").last, localFileUnzip )
            localFile.delete()
            localFileUnzip.delete()
            
})

Commande awscli magique
%sh aws s3 ls --summarize --human-readable --recursive s3://rods3/

import java.util.zip.ZipFile
import java.io.FileInputStream
import java.io.FileOutputStream
import scala.collection.JavaConversions._
import java.io.InputStream
import java.io.OutputStream
import java.io.File
import java.util.zip.ZipEntry

class ZipArchive {

  val BUFSIZE = 4096
  val buffer = new Array[Byte](BUFSIZE)

  def unZip(source: String, targetFolder: String) = {
    FileIsExist(source)
    {
    	val zipFile = new ZipFile(source)

    	unzipAllFile(zipFile.entries.toList, getZipEntryInputStream(zipFile)_, new File(targetFolder))
    }
  }

  def FileIsExist(path:String)(expr: => Any) = {
    if(new File(path).exists)
      expr
  }
  
  /*---------------------------------------------------------------------------------
     * curry method , this methond can get the inputstream of a zip entry from zipFile
     *---------------------------------------------------------------------------------*/
  def getZipEntryInputStream(zipFile: ZipFile)(entry: ZipEntry) = zipFile.getInputStream(entry)

  def unzipAllFile(entryList: List[ZipEntry], getInputStream: (ZipEntry) => InputStream, targetFolder: File): Boolean = {

    entryList match {
      case entry :: entries =>

        if (entry.isDirectory)
          new File(targetFolder, entry.getName).mkdirs
        else
          saveFile(getInputStream(entry), new FileOutputStream(new File(targetFolder, entry.getName)))

        unzipAllFile(entries, getInputStream, targetFolder)

      case _ =>
        true
    }
  }

  /*=============================================================
   * 
   * Read InputStream and write the data to OutputStream
   * the recursive method is writeToFile and bufferReader
   *
   *=============================================================*/
  def saveFile(fis: InputStream, fos: OutputStream) = {

      /*--------------------------------------------------------------
	   * curry a method , the method can read data from InputStream
	   *--------------------------------------------------------------*/
      def bufferReader(fis: InputStream)(buffer: Array[Byte]) = (fis.read(buffer), buffer)

      /*--------------------------------------------------------------
	   * Write the data in the buffer to outputstream
	   ---------------------------------------------------------------*/
      def writeToFile(reader: (Array[Byte]) => Tuple2[Int, Array[Byte]], fos: OutputStream): Boolean = {
        val (length, data) = reader(buffer)
        if (length >= 0) {
          fos.write(data, 0, length)
          writeToFile(reader, fos)
        } else
          true
      }

    try {
      writeToFile(bufferReader(fis)_, fos)
    } finally {
      fis.close
      fos.close
    }
  }

}

